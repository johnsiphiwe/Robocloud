{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOobNOmGr4JxrIyLjeF05Kl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johnsiphiwe/Robocloud/blob/main/News_Scrapper2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrjp3y1doV3q"
      },
      "outputs": [],
      "source": [
        "# News Scrapper\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://www.reuters.com/'\n"
      ],
      "metadata": {
        "id": "gL48yQ1aodc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_news(url):\n",
        "    # Send a GET request to the URL\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Check if the request was successful\n",
        "    if response.status_code == 200:\n",
        "        # Parse the HTML content of the page\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Find all the news article elements (you may need to inspect the HTML of the website to find the correct tags)\n",
        "        news_articles = soup.find_all('div', class_='news-article')\n",
        "\n",
        "        # Initialize lists to store headlines and URLs\n",
        "        headlines = []\n",
        "        article_urls = []\n",
        "\n",
        "        # Loop through each news article element\n",
        "        for article in news_articles:\n",
        "            # Extract headline\n",
        "            headline = article.find('h2').text.strip()\n",
        "            headlines.append(headline)\n",
        "\n",
        "            # Extract URL\n",
        "            url = article.find('a')['href']\n",
        "            article_urls.append(url)\n",
        "\n",
        "        # Return the extracted headlines and URLs\n",
        "        return headlines, article_urls\n",
        "    else:\n",
        "        print(\"Failed to fetch the page:\", response.status_code)\n",
        "        return [], []\n",
        "\n",
        "url = 'https://www.reuters.com/'\n",
        "headlines, article_urls = scrape_news(url)\n",
        "\n",
        "# Print the extracted headlines and URLs\n",
        "for headline, article_url in zip(headlines, article_urls):\n",
        "    print(\"Headline:\", headline)\n",
        "    print(\"URL:\", article_url)\n",
        "    print()\n",
        "\n",
        "    # PEP-8 compliant"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NP-JJJUnorbl",
        "outputId": "69176dfd-27df-43ee-ca74-458db869b2d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to fetch the page: 401\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S8yH6iI9owx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8FwUS4cXpcZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tnW1sgEWpecv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OosAla1IpgR3",
        "outputId": "3d34b169-13de-4658-dded-182bc4eb6cc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty DataFrame\n",
            "Columns: [Title, Summary]\n",
            "Index: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(news_data.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDZJ4AkFph4O",
        "outputId": "3524f086-eb71-4fab-a17d-9f244aedafc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eT-e1dEiu2qw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}